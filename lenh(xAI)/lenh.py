
import streamlit as st
import torch
import torch.nn as nn
from captum.attr import IntegratedGradients
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import mne
import os
import tempfile

# ==============================================================================
# 0Ô∏è‚É£ Device & Model (S·ª¨ D·ª§NG M√î H√åNH C·ª¶A B·∫†N V·ªöI TR·ªåNG S·ªê ƒê√É T·∫¢I)
# ==============================================================================

# D√°n ƒë·ªãnh nghƒ©a m√¥ h√¨nh c·ªßa b·∫°n v√†o ƒë√¢y
class EEGCNN1D(nn.Module):
    def __init__(self, n_channels=32, n_classes=2):
        super().__init__()
        self.conv1 = nn.Conv1d(n_channels, 64, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm1d(64)
        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)
        self.bn2 = nn.BatchNorm1d(128)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(128, n_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.pool(x).squeeze(-1)
        x = self.fc(x)
        return x

# --- C·∫•u h√¨nh v√† kh·ªüi t·∫°o model ---
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ### <<< S·ª¨A L·ªñI T·∫†I ƒê√ÇY >>> ###
# THAY ƒê·ªîI S·ªê K√äNH T·ª™ 32 TH√ÄNH 40 ƒê·ªÇ KH·ªöP V·ªöI MODEL ƒê√É HU·∫§N LUY·ªÜN
N_CHANNELS_MODEL_EXPECTS = 40
N_CLASSES = 2

# Kh·ªüi t·∫°o ki·∫øn tr√∫c model
model = EEGCNN1D(n_channels=N_CHANNELS_MODEL_EXPECTS, n_classes=N_CLASSES)

MODEL_PATH = "eeg_cnn_model_weights.pth"
try:
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    st.session_state.model_loaded = True
except FileNotFoundError:
    st.session_state.model_loaded = False
except Exception as e:
    # Th√™m m·ªôt th√¥ng b√°o l·ªói c·ª• th·ªÉ h∆°n tr√™n web
    st.error(f"L·ªñI NGHI√äM TR·ªåNG KHI T·∫¢I MODEL: {e}")
    st.error("R·∫•t c√≥ th·ªÉ s·ªë k√™nh `N_CHANNELS_MODEL_EXPECTS` trong code kh√¥ng kh·ªõp v·ªõi s·ªë k√™nh c·ªßa model ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán. Vui l√≤ng ki·ªÉm tra l·∫°i.")
    st.stop()


model.to(device)
model.eval()

# ==============================================================================
# C√ÅC PH·∫¶N C√íN L·∫†I C·ª¶A FILE GI·ªÆ NGUY√äN
# (Copy-paste to√†n b·ªô c√°c h√†m process_eeg, plot_attributions, v√† giao di·ªán Streamlit v√†o ƒë√¢y)
# ==============================================================================

# 1Ô∏è‚É£ C·∫•u h√¨nh chung
channels_of_interest = ['C3','Cz','C4','F3','F4','P3','P4']
n_steps_ig = 50
resample_rate = 128
cmap = plt.cm.inferno

# 2Ô∏è‚É£ H√†m x·ª≠ l√Ω EEG + Integrated Gradients
def process_eeg(file_path, model, device, n_steps, resample_rate, n_channels_expected):
    raw = mne.io.read_raw_bdf(file_path, preload=True, verbose=False).crop(0,30)
    n_chans_raw = len(mne.pick_types(raw.info, eeg=True))
    raw.filter(1., 40., fir_design='firwin', verbose=False)
    ica = mne.preprocessing.ICA(n_components=10, random_state=97, max_iter=800, verbose=False)
    ica.fit(raw)
    eog_indices, _ = ica.find_bads_eog(raw, ch_name='Fp1')
    ica.exclude = eog_indices
    raw_clean = ica.apply(raw.copy(), verbose=False)
    raw_eeg = raw_clean.copy().pick_types(eeg=True)
    eeg_data_original = raw_eeg.get_data()
    eeg_data_adjusted = eeg_data_original
    if n_chans_raw != n_channels_expected:
        if n_chans_raw < n_channels_expected:
            repeat_factor = (n_channels_expected // n_chans_raw) + 1
            eeg_data_adjusted = np.tile(eeg_data_original, (repeat_factor, 1))[:n_channels_expected, :]
        else:
            eeg_data_adjusted = eeg_data_original[:n_channels_expected, :]
    x_tensor = torch.tensor(eeg_data_adjusted, dtype=torch.float32).unsqueeze(0).to(device)
    with torch.no_grad():
        outputs = model(x_tensor)
        probabilities = torch.softmax(outputs, dim=1)
        confidence, predicted_class_idx = torch.max(probabilities, 1)
    ig = IntegratedGradients(model)
    attr = ig.attribute(x_tensor, target=predicted_class_idx.item(), n_steps=n_steps).detach().cpu().numpy()[0]
    attr = attr[:n_chans_raw, :]
    raw_resampled = raw_eeg.copy().resample(resample_rate)
    x_data_resampled = raw_resampled.get_data()
    factor = eeg_data_original.shape[1] // x_data_resampled.shape[1]
    attr_ds = attr[:, ::factor]
    attr_norm = np.abs(attr_ds)**0.5
    max_attr = attr_norm.max()
    if max_attr > 0:
        attr_norm /= max_attr
    return x_data_resampled, attr_norm, raw_resampled.ch_names, predicted_class_idx.item(), confidence.item(), n_chans_raw

# 3Ô∏è‚É£ H√†m v·∫Ω bi·ªÉu ƒë·ªì
def plot_attributions(file_name, x_data, attr_data, ch_names, channels_of_interest):
    n_per_fig = len(channels_of_interest)
    plt.style.use('default')
    fig, axes = plt.subplots(n_per_fig, 1, figsize=(18, 2.5 * n_per_fig), sharex=True)
    if n_per_fig == 1: axes = [axes]
    for j, ch_name in enumerate(channels_of_interest):
        ax = axes[j]
        if ch_name in ch_names:
            ch_idx = ch_names.index(ch_name)
            y_signal, attr_signal = x_data[ch_idx], attr_data[ch_idx]
            segments = np.array([[[k, y_signal[k]], [k+1, y_signal[k+1]]] for k in range(y_signal.shape[0]-1)])
            lc = LineCollection(segments, colors=cmap(attr_signal[:-1]), linewidths=1.5)
            ax.add_collection(lc)
            y_range = y_signal.max() - y_signal.min()
            padding = y_range * 0.1
            ax.set_xlim(0, y_signal.shape[0]); ax.set_ylim(y_signal.min() - padding, y_signal.max() + padding)
            ax.set_ylabel(ch_name, fontsize=12); ax.grid(True, linestyle='--', alpha=0.4)
        else:
            ax.set_ylabel(f"{ch_name}\n(not found)")
            ax.text(0.5, 0.5, 'K√™nh kh√¥ng c√≥ trong file', ha='center', va='center', transform=ax.transAxes)
    axes[-1].set_xlabel("Th·ªùi gian (M·∫´u)", fontsize=12)
    fig.suptitle(f"Ph√¢n T√≠ch AI Tr√™n T√≠n Hi·ªáu EEG\nFile: {file_name}", fontsize=18)
    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=1)); sm.set_array([])
    cbar = fig.colorbar(sm, ax=axes, orientation='vertical', fraction=0.02, pad=0.01)
    cbar.set_label('M·ª©c ƒê·ªô Quan Tr·ªçng (Integrated Gradients)', fontsize=12)
    plt.tight_layout(rect=[0, 0, 0.98, 0.95]); return fig

# 4Ô∏è‚É£ X√¢y d·ª±ng giao di·ªán Streamlit
st.set_page_config(layout="wide")
st.title("üî¨ ·ª®ng d·ª•ng Ph√¢n t√≠ch v√† Gi·∫£i th√≠ch AI cho EEG")

if 'model_loaded' not in st.session_state or not st.session_state.model_loaded:
    st.error(f"Kh√¥ng t√¨m th·∫•y file tr·ªçng s·ªë '{MODEL_PATH}'. Vui l√≤ng ƒë·∫£m b·∫£o b·∫°n ƒë√£ l∆∞u model v√† file n√†y t·ªìn t·∫°i trong m√¥i tr∆∞·ªùng Colab.")
    st.stop()

if st.session_state.get('model_loaded', False):
  st.success(f"ƒê√£ t·∫£i th√†nh c√¥ng model t·ª´ file '{MODEL_PATH}'.")

st.markdown(f"""
·ª®ng d·ª•ng n√†y s·ª≠ d·ª•ng m√¥ h√¨nh **`{model.__class__.__name__}`** ƒë·ªÉ d·ª± ƒëo√°n tr√™n d·ªØ li·ªáu EEG.
- **Model mong ƒë·ª£i ƒë·∫ßu v√†o c√≥ `{N_CHANNELS_MODEL_EXPECTS}` k√™nh.**
- **Gi·∫£i th√≠ch (XAI)** ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng k·ªπ thu·∫≠t **Integrated Gradients**.
""")

st.header("1. T·∫£i l√™n file EEG (.bdf)")
uploaded_file = st.file_uploader("Ch·ªçn m·ªôt file .bdf", type=["bdf"])

if uploaded_file is not None:
    with tempfile.NamedTemporaryFile(delete=False, suffix='.bdf') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
    st.info(f"ƒê√£ t·∫£i l√™n file: **{uploaded_file.name}**. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
    try:
        raw_info = mne.io.read_raw_bdf(tmp_file_path, preload=False, verbose=False)
        n_chans_in_file = len(mne.pick_types(raw_info.info, eeg=True))
        if n_chans_in_file != N_CHANNELS_MODEL_EXPECTS:
            st.warning(f"‚ö†Ô∏è **C·∫£nh b√°o:** Model ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi **{N_CHANNELS_MODEL_EXPECTS}** k√™nh, nh∆∞ng file c·ªßa b·∫°n c√≥ **{n_chans_in_file}** k√™nh EEG. ·ª®ng d·ª•ng s·∫Ω t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh d·ªØ li·ªáu, nh∆∞ng k·∫øt qu·∫£ c√≥ th·ªÉ kh√¥ng t·ªëi ∆∞u.")
        with st.spinner('ƒêang ch·∫°y m√¥ h√¨nh AI v√† t√≠nh to√°n XAI... Vui l√≤ng ch·ªù.'):
            x_data, attr_data, ch_names, pred_idx, conf, _ = process_eeg(tmp_file_path, model, device, n_steps_ig, resample_rate, N_CHANNELS_MODEL_EXPECTS)
            fig = plot_attributions(uploaded_file.name, x_data, attr_data, ch_names, channels_of_interest)
        st.success("X·ª≠ l√Ω ho√†n t·∫•t!")
        st.header("2. K·∫øt qu·∫£ d·ª± ƒëo√°n")
        class_names = [f"L·ªõp {i}" for i in range(N_CLASSES)]
        st.metric(label="D·ª± ƒëo√°n c·ªßa m√¥ h√¨nh", value=class_names[pred_idx])
        st.progress(conf)
        st.write(f"ƒê·ªô tin c·∫≠y (Confidence): **{conf:.2%}**")
        st.header("3. Ph√¢n t√≠ch Explainable AI (XAI)")
        st.pyplot(fig)
    except Exception as e:
        st.error(f"ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")
        st.error("Vui l√≤ng ki·ªÉm tra ƒë·ªãnh d·∫°ng file BDF. K√™nh 'Fp1' ph·∫£i t·ªìn t·∫°i ƒë·ªÉ thu·∫≠t to√°n ICA lo·∫°i b·ªè nhi·ªÖu m·∫Øt c√≥ th·ªÉ ho·∫°t ƒë·ªông.")
    finally:
        os.remove(tmp_file_path)
else:
    st.info("Vui l√≤ng t·∫£i l√™n m·ªôt file .bdf ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
